# Exercise 1: Orange juice dataset

The `OJ` data from the `ISLR` package contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. A number of characteristics of the customer and product are recorded.

* `Purchase`: A factor with levels CH and MM indicating whether the customer purchased Citrus Hill or Minute Maid Orange Juice (i.e. response/class)
* `WeekofPurchase`: Week of purchase
* `StoreID`: Store ID
* `PriceCH`: Price charged for CH
* `PriceMM`: Price charged for MM
* `DiscCH`: Discount offered for CH
* `DiscMM`: Discount offered for MM
* `SpecialCH`: Indicator of special on CH
* `SpecialMM`: Indicator of special on MM
* `LoyalCH`: Customer brand loyalty for CH
* `SalePriceMM`:Sale price for MM
* `SalePriceCH`: Sale price for CH
* `PriceDiff`: Sale price of MM less sale price of CH
* `Store7`: A factor with levels No and Yes indicating whether the sale is at Store 7
* `PctDiscMM`: Percentage discount for MM
* `PctDiscCH`: Percentage discount for CH
* `ListPriceDiff`: List price of MM less list price of CH
* `STORE`: Which of 5 possible stores the sale occured at

The question of interest is to predict which brand of orange juice a customer will purchase (`Purchase` variable).

Source: Stine, Robert A., Foster, Dean P., Waterman, Richard P. Business Analysis Using Regression (1998). Published by Springer.

To load the data, use
```{r}
library(ISLR)
data(OJ)
```

## Exploratory data analysis

In `R`, categorical variables are sometimes recorded as numerical variables, which is inappropriate for subsequent analysis. To address this, `as.factor()` should be used to convert these variables back into categorical variables. 

**Task**

1. Use `str`, `summary` and/or `skim` to inspect the data. Looking at the variable description above, is there any categorical variable encoded incorrectly as numerical variable? If so, correct them by using `as.factor()`. 

Excluding the response variable `Purchase`, how many variables are numerical variables and how many variables are categorical variables?

* The number of numerical variables = `r fitb(12)`.
* The number of categorical variables = `r fitb(5)`.


```{r, webex.hide="Solution"}
library(skimr)
str(OJ)
skim(OJ)
OJ$StoreID <- as.factor(OJ$StoreID)
OJ$SpecialCH <- as.factor(OJ$SpecialCH)
OJ$SpecialMM <- as.factor(OJ$SpecialMM)
OJ$STORE <- as.factor(OJ$STORE)
str(OJ)
skim(OJ)
```

2. Use `ggplot` or `ggpairs` to visualise numerical variables, answering questions such as which variables may be useful in predicting the type of orange juice purchased by the customer, any outlier/extreme observations.

`r hide("Hint")`
Check Lab 3 Sections 2.1 and 3.2 for examples. 
`r unhide()`

`r hide("Solution")`
Below is an example code examining `PriceDiff` and `ListPriceDiff` using density plots and boxplots. 
```{r}
library(ggplot2)
ggplot(OJ, aes(x=PriceDiff, colour=Purchase)) +
  geom_density()
ggplot(OJ, aes(x=PriceDiff, colour=Purchase)) +
  geom_boxplot()
library(GGally)
ggpairs(OJ, columns=c(13,17), ggplot2::aes(colour=Purchase, alpha=0.2))
```
`r unhide()`

3. Produce boxplots of categorical variables separated by groups. Check the helppage of `boxplot()` or use `geom_boxplot()` in `ggplot`. 

`r hide("Solution")`
Below is an example code examining `SpecialCH` using barchart. 
```{r}
ggplot(OJ, aes(x=SpecialCH, fill=Purchase)) +
  geom_bar()
```
`r unhide()`

## Effect of different parameters in SVM

Before building a powerful SVM classifier, we illustrate the effect of different kernels and/or different parameters in SVM by working with only two numeric variables (hence we could visualise the decision boundary in 2D). 

Suppose we select `PriceDiff` and `ListPriceDiff` as the predictor variables. 

**Task**

4. Build a support vector classifier, i.e. SVM with a linear kernel, and visualise the decision boundary. 
```{r, webex.hide="Solution"}
library(e1071)
Model_linear <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="linear")
plot(Model_linear, OJ, ListPriceDiff~PriceDiff)
```

5. Build an SVM with a polynomial kernel with two different degrees (by using the argument `degree`) and two intercept coefficients (by using the argument `coef`). 

The decision boundary will be `r mcq(c("less flexible", answer="more flexible"))` if the degree in the polynomial kernel is higher. 

```{r, webex.hide="Solution"}
Model_poly <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=2)
plot(Model_poly, OJ, ListPriceDiff~PriceDiff)

Model_poly2 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=4)
plot(Model_poly2, OJ, ListPriceDiff~PriceDiff)

Model_poly3 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=4, coef=2)
plot(Model_poly3, OJ, ListPriceDiff~PriceDiff)
```

6. Now fix the degree and intercepts in the polynomial kernel and change the cost parameter for soft-margin SVM (by using the argument `cost`). Visualise the decision boundary for two different cost values.

```{r, webex.hide="Solution"}
Model_poly4 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=5, cost=0.01)
plot(Model_poly4, OJ, ListPriceDiff~PriceDiff)

Model_poly5 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=5, cost=10)
plot(Model_poly5, OJ, ListPriceDiff~PriceDiff)
```


<!-- Note that in `e1071` package, SVM is formulated using the dual optimisation problem (see the supplementary material on page 6 of Week 5 note). A consequence of this is that the cost parameter in `R` takes the opposite effect to the cost parameter $C$ taught in the lecture (i.e. the penalty assigned to slack variables). In other words, a small value of cost parameter in `R` corresponds to a large value of $C$ in the lecture note, and vice versa. We can check this by comparing the classification accuracy of SVMs built with different cost parameters. Intuitively, a large $C$ (i.e. a small cost in `R`) encourages the model to better fit the data and hence improve the accuracy, and this is indeed the case as shown in the `R` command below.  -->

<!-- ```{r} -->
<!-- Model_poly3 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=5, cost=0.01) -->
<!-- mean(OJ$Purchase == predict(Model_poly3,OJ)) -->
<!-- Model_poly4 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="poly", degree=5, cost=10) -->
<!-- mean(OJ$Purchase == predict(Model_poly4,OJ)) -->
<!-- ``` -->

7. Build an SVM with a radial basis kernel with two different gamma values (using the argument `gamma`) and visualise the decision boundary. 

The decision boundary will be `r mcq(c("less flexible", answer="more flexible"))` if gamma in the radial basis function kernel is larger. 
```{r, webex.hide="Solution"}
Model_RBF <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="radial", gamma=0.1)
plot(Model_RBF, OJ, ListPriceDiff~PriceDiff)

Model_RBF2 <- svm(Purchase~PriceDiff+ListPriceDiff, OJ, type="C-classification", kernel="radial", degree=10)
plot(Model_RBF2, OJ, ListPriceDiff~PriceDiff)
```


## Build an SVM

Now we start to build an SVMly build an SM

```{r}
sapply(OJ,is.numeric)
```


